{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Titanic with Naive Bayes Classifier from scratch (without using Scikit-learn)**\nA fundamental part of learning ML is understanding the theory behind every used statistical model. Otherwise, Scikit-learn is nothing but a magical black box to the user. The Naive Bayes algorithm is quite simple and fun to start with. It will be explained and used in the Titanic Survival prediction problem, using Python. \n\n**Note:** My goal is to show the basics of the algorithm and how I implemented it, so I did not put much effort in feature engineering to obtain the highest score possible.\n\n## Theory: Bayes Theorem\nIt describes the probability of an event, based on prior knowledge of conditions that might be related to the event. Using Bayes theorem, we can find the probability of \\\\( y \\\\) happening, given that \\\\( x \\\\) has occurred. Here, \\\\( x \\\\) is the evidence, \\\\( y \\\\) is the prior knowledge and \\\\( P(x|y) \\\\) is the likelihood. The assumption made here is that the predictors/features are independent.\n\nIt's equation is as follows:\n\n\\\\( P(y|x)= \\dfrac{P(x|y)P(y)}{P(x)} \\\\)\nwhere \n* \\\\( y,x \\\\) = Events\n* \\\\( P(y|x) \\\\) = Probability of \\\\( y \\\\) given \\\\( x \\\\)\n* \\\\( P(x|y) \\\\) = Probability of \\\\( x \\\\) given \\\\( y \\\\)\n* \\\\( P(y), P(x) \\\\) = Independent probabilities of \\\\( y \\\\) and \\\\( x \\\\)\n\nNaive Bayes Methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable.\n\nIn our case, the variable \\\\( y \\\\) is the class variable (Survival 0 or 1), which represents if a passenger will survive or not given the conditions. Variable \\\\( X \\\\) represent the parameters/features. \\\\( X \\\\) is given as \\\\( X=(x_{1}, x_{2}, ..., x_{n}) \\\\) and they can be mapped to Age, Class, Sex, etc. By substituting for \\\\( X \\\\) into the Bayes Rule and expanding using the chain rule we get\n        \n\\\\( P(y|x_{1}, x_{2}, ..., x_{n})= \\dfrac{P(x_{1}|y)P(x_{2}|y)...P(x_{n}|y)P(y)}{P(x_{1})P(x_{2})...P(x_{n})} \\\\)\n\nNow, you can calculate the values for each probability by looking at the dataset and substitute them into the equation. In our case, the class variable \\\\( y \\\\) has two outcomes: 0 or 1. So the survival probabilities for each case '*Survival*' or '*No survival*' (1 or 0, respectively) needs to be calculated for each passenger. The one having the highest probability is then the final outcome. That is, if \\\\( P(1) > P(0) \\\\), the person will survive.\n\n\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\n\n\ndf_train=pd.read_csv('../input/titanic/train.csv')\ndf_test=pd.read_csv('../input/titanic/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some feature engineering below (for simplicity I used 3 features only: Class, Sex and Age):"},{"metadata":{"trusted":true},"cell_type":"code","source":"sexos={\"male\":0, \"female\":1}\ndf_train.Sex=[sexos[item] for item in df_train.Sex]\ndf_test.Sex=[sexos[item] for item in df_test.Sex]\n\ndf_train.Age.fillna(df_train.Age.mean(), inplace=True)\ndf_test.Age.fillna(df_test.Age.mean(), inplace=True)\n\ndf_train.Age=df_train.Age.astype(int)\ndf_test.Age=df_test.Age.astype(int)\n\n#A wild plot has appeared, just for the heck of it\nsns.violinplot(x='Pclass', y='Age', hue='Survived', data=df_train, split=True)\n\n#Ages grouped\ndata = [df_train, df_test]\nfor dataset in data:\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 7","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Machine Learning model construction starts here:\n\nWe have 3 features, so the Bayes rule takes the following form,\n\n\\\\( P(y|x_{1}, x_{2},x_{3})= \\dfrac{P(x_{1}|y)P(x_{2}|y)P(x_{3}|y)P(y)}{P(x_{1})P(x_{2})P(x_{3})} \\\\)\n\nwhere \n\n\\\\( P(y) \\\\) = Probability of survival (for 0 and for 1), so it is a 2-dimensional array.\n* \\\\( P(x_{1}) \\\\) = Probability of Pclass, it is a 3-dimensional array (denoted as p_Class in the code)\n* \\\\( P(x_{2}) \\\\) = Probability of gender, 2-dimensional array (denoted as p_Sex in the code)\n* \\\\( P(x_{3}) \\\\) = Probability of Age, 8-dimensional array (denoted as p_Age in the code)\n\nand the conditional probabilities\n\n*  \\\\( P(x_{1}|y) \\\\) = Probability of Pclass given survival (0 or 1)\n*  \\\\( P(x_{2}|y) \\\\) = Probability of gender given survival (0 or 1)\n*  \\\\( P(x_{3}|y) \\\\) =  Probability of Age given survival (0 or 1)\n\nThe probabilities are calculated below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#probabilities of the features\n    \nClass_counts=df_train['Pclass'].value_counts()  \np_Class=Class_counts/len(df_train)\n\nSex_counts=df_train['Sex'].value_counts()\np_Sex=Sex_counts/len(df_train)\n\nAge_counts=df_train['Age'].value_counts()\np_Age=Age_counts/len(df_train)\n\n# Survival and Death probabilities\ny_counts=df_train['Survived'].value_counts()\np_y=y_counts/len(df_train)\n\ndf_survived=df_train.loc[df_train['Survived'] == 1]\ndf_died=df_train.loc[df_train['Survived'] == 0]\n\n# Conditional probabilities\n#class\nclass_survived_counts=df_survived['Pclass'].value_counts()  \np_class_survived=class_survived_counts/len(df_survived)\n\nclass_died_counts=df_died['Pclass'].value_counts()  \np_class_died=class_died_counts/len(df_died)\n\n#sex\nsex_survived_counts=df_survived['Sex'].value_counts()  \np_sex_survived=sex_survived_counts/len(df_survived)\n\nsex_died_counts=df_died['Sex'].value_counts()  \np_sex_died=sex_died_counts/len(df_died)\n\n#Age\nage_survived_counts=df_survived['Age'].value_counts()  \np_age_survived=age_survived_counts/len(df_survived)\n\nage_died_counts=df_died['Age'].value_counts()  \np_age_died=age_died_counts/len(df_died)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bayes function defined below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Bayes(py, px1y, px2y, px3y, px1, px2, px3):\n    numerator=px1y*px2y*px3y*py\n    denominator=px1*px2*px3\n    p=numerator/denominator\n    return p","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The probabilities of survival for each passenger calculated below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"result_array=[]\n\nfor i in range(0,418):\n    feature_class=df_test.iloc[i]['Pclass']\n    feature_sex=df_test.iloc[i]['Sex']\n    feature_age=df_test.iloc[i]['Age']\n    \n    P_Y1=Bayes(p_y[1], p_class_survived[feature_class], p_sex_survived[feature_sex], p_age_survived[feature_age], p_Class[feature_class], p_Sex[feature_sex], p_Age[feature_age])\n    P_Y0=Bayes(p_y[0], p_class_died[feature_class], p_sex_died[feature_sex], p_age_died[feature_age], p_Class[feature_class], p_Sex[feature_sex], p_Age[feature_age])\n    \n    if P_Y0 > P_Y1:\n        result=0\n    else:\n        result=1\n        \n    result_array.append(result)\n\n\noutput = pd.DataFrame({'PassengerId': df_test.PassengerId,'Survived': result_array})\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission:\n\nThis predictor scored **0.77033**, which is not that bad for a model **without using Scikit-learn library!!!!!!**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}